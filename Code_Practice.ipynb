{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c935b7de",
   "metadata": {},
   "source": [
    "### Implementing an Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99168a10",
   "metadata": {},
   "source": [
    "- Import the scikit-learn library for linear regression.\n",
    "- Instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d17fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SciKitLearn Algorithm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate the model\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1bd770",
   "metadata": {},
   "source": [
    "### Training the Model in Practice\n",
    "\n",
    "- Read in the CSV file for the dataset.\n",
    "- Organize and structure the data in a DataFrame.\n",
    "- Select X & y training subsets.\n",
    "- Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b044a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas read csv function.\n",
    "from pandas import DataFrame as DF, read_csv\n",
    "\n",
    "# Read in the training data csv file.\n",
    "dataset = read_csv(\"Training the Model in Practice/no_null_encoded_titanic.csv\")\n",
    "\n",
    "# Structure the data for training the model.\n",
    "dataset = dataset.drop(columns=[\"Unnamed: 0.1\", \"Unnamed: 0\", \"alive\"])\n",
    "\n",
    "\"\"\"\n",
    "FOR THESE IT MAY BE MORE STURDY TO REFERENCE BY NAME IN CASE SOMETHING HAPPENS WHERE INDEXES ARE SWITCHED BY ACCIDENT\n",
    "INDEX REFERENCE CAN BE MORE ACCIDENT PRONE\n",
    "\"\"\"\n",
    "\n",
    "# Create Subsets of the dataset to train the model on. (X & y selection)\n",
    "target_column_name = \"survived\"\n",
    "y_training = dataset[target_column_name]\n",
    "X_training = dataset.drop(columns=target_column_name)\n",
    "\n",
    "# View the training subsets.\n",
    "#display(X_training)\n",
    "#display(y_training)\n",
    "\n",
    "# Train the model on the subsets of data that were created.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e63420",
   "metadata": {},
   "source": [
    "### Applying a Model\n",
    "\n",
    "- Setup and train a classification model using the Titanic dataset.\n",
    "- Setup and train a Regression model using the House cleaning dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222a0e9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [714, 8, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m DT_target_data_feature = \u001b[33m\"\u001b[39m\u001b[33msurvived\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m LR_target_data_feature = \u001b[33m\"\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m train, test = \u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDT_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDT_target_data_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m<\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m<\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m<\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m<\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     42\u001b[39m X_train = train.drop(DT_target_data_feature)\n\u001b[32m     43\u001b[39m y_train = train[DT_target_data_feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2916\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_arrays == \u001b[32m0\u001b[39m:\n\u001b[32m   2914\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one array required as input\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2916\u001b[39m arrays = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2918\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m   2919\u001b[39m n_train, n_test = _validate_shuffle_split(\n\u001b[32m   2920\u001b[39m     n_samples, test_size, train_size, default_test_size=\u001b[32m0.25\u001b[39m\n\u001b[32m   2921\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:530\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    501\u001b[39m \n\u001b[32m    502\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [714, 8, 1]"
     ]
    }
   ],
   "source": [
    "# Import the Decision Tree and Linear Regression Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "# Import pandas functions.\n",
    "from pandas import concat\n",
    "import pandas as pd\n",
    "\n",
    "# Set pandas options\n",
    "pd.options.display.float_format = '{:10,.2f}'.format\n",
    "\n",
    "# Variables\n",
    "training_percentage = 0.75\n",
    "# TRAIN TEST SPLIT IS SIMPLEST METHOD FOR ... TRAIN TEST SPLIT\n",
    "# NO NEED TO INIT VARS IN PYTHON\n",
    "\n",
    "# Instantiate the models.\n",
    "DT_Model = DecisionTreeClassifier()\n",
    "LR_Model = LinearRegression()\n",
    "\n",
    "# Read in the Datasets\n",
    "DT_dataset = read_csv(\"Applying a Model/no_null_encoded_titanic.csv\")\n",
    "LR_dataset = read_csv(\"Applying a Model/kc_house_cleaned (2).csv\")\n",
    "\n",
    "# Display the datasets\n",
    "#display(DT_dataset)\n",
    "#display(LR_dataset)\n",
    "\n",
    "# Structure and organize the datasets.\n",
    "DT_dataset = DT_dataset.drop(columns=[\"Unnamed: 0.1\", \"Unnamed: 0\", \"alive\"])\n",
    "LR_dataset = LR_dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "# INDEX BASED COLUMN REFERENCE IS RISKY SINCE INDEXES CAN SOMETIMES CHANGE BY ACCIDENT DURING DATASET MANIPULATION\n",
    "# TEXT LABEL BASED INDEXING ADAPTS TO ANY INDEX CHANGES THAT MIGHT OCCUR IF YOU NEEDED TO REFACTOR THE CODE\n",
    "\n",
    "# Set target features\n",
    "DT_target_data_feature = \"survived\"\n",
    "LR_target_data_feature = \"price\"\n",
    "\n",
    "train, test = tts(DT_dataset, DT_target_data_feature, [\"\"<\"\"<\"\"<\"\"<\"\"]) \n",
    "\n",
    "X_train = train.drop(DT_target_data_feature)\n",
    "y_train = train[DT_target_data_feature]\n",
    "\n",
    "# Slice datasets for training and validation.\n",
    "DT_pivot = round(len(DT_dataset.index)*training_percentage)\n",
    "LR_pivot = round(len(LR_dataset.index)*training_percentage)\n",
    "DT_train = DT_dataset.iloc[:DT_pivot]\n",
    "DT_test = DT_dataset.iloc[DT_pivot:]\n",
    "LR_train = LR_dataset.iloc[:LR_pivot]\n",
    "LR_test = LR_dataset.iloc[LR_pivot:]\n",
    "# SEVERAL SHORTCUTS HERE:\n",
    "    #index[DT_pivot:len(DT_dataset.index)]\n",
    "        # =>\n",
    "        # index[DT_pivot:]\n",
    "    # index[0:DT_pivot]\n",
    "        # =>\n",
    "        # index[:DT_pivot]\n",
    "    # DT_dataset.drop(DT_dataset.index[DT_pivot:len(DT_dataset.index)])\n",
    "        # =>\n",
    "        # DT_dataset.iloc[DT_pivot:len(DT_dataset.index)]\n",
    "            # =>\n",
    "            # DT_dataset.iloc[DT_pivot:]\n",
    "\n",
    "# Build subsets for validation.\n",
    "X_DT_Testing = DT_test.drop(columns=DT_target_data_feature)\n",
    "\n",
    "y_DT_Testing = DF(DT_test[DT_target_data_feature]) \n",
    "# SHOULDN'T NORMALLY BE NECESSARY TO RECAST AS A DATAFRAME IN THIS PARTICULAR CONTEXT - MAY WANT TO CHECK OUT 1-1 IF THIS DOES CRASH\n",
    "y_DT_Testing.rename(columns={DT_target_data_feature:\"Known Answers\"}, inplace=True)\n",
    "y_DT_Testing.reset_index(drop=True, inplace=True)\n",
    "# RESETTING INDEX GENERALLY ISN'T NECESSARY UNLESS YOU ARE COMBINING WITH A PREDICTION SET TO COMPARE ANSWERS AT THE END\n",
    "# HOWEVER IF YOU ARE GOING TO COMBINE WITH A PREDICTION SET FOR SIDE-BY-SIDE COMPS, THEN IT IS NECESSARY, SO GOOD JOB THERE\n",
    "# BUT THEN THE RELATION OF THE LABELS TO THE X (INPUTS) IS LOST, IF YOU WANT THOSE. JUST DEPENDS.\n",
    "\n",
    "X_LR_Testing = LR_test.drop(columns=LR_target_data_feature)\n",
    "\n",
    "y_LR_Testing = DF(LR_test[LR_target_data_feature])\n",
    "#y_LR_Testing.rename(columns={LR_target_data_feature:\"Known Answers\"}, inplace=True)\n",
    "y_LR_Testing.reset_index(drop=True, inplace=True)\n",
    "# SAME COMMENTS AS ABOVE\n",
    "\n",
    "# Build subsets for training.\n",
    "X_DT_Training = DT_train.drop(columns=DT_target_data_feature)\n",
    "y_DT_Training = DT_train[DT_target_data_feature]\n",
    "\n",
    "X_LR_Training = LR_train.drop(columns=LR_target_data_feature)\n",
    "y_LR_Training = LR_train[LR_target_data_feature]\n",
    "\n",
    "# Fit models with the training subsets.\n",
    "DT_Model.fit(X_DT_Training, y_DT_Training)\n",
    "LR_Model.fit(X_LR_Training, y_LR_Training)\n",
    "\n",
    "# Test Models with testing subsets.\n",
    "DT_result = DT_Model.predict(X_DT_Testing)\n",
    "DT_result = DF(DT_result, columns=[\"Predictions\"])\n",
    "\n",
    "LR_result = LR_Model.predict(X_LR_Testing)\n",
    "#LR_result = DF(LR_result, columns=[\"Predictions\"])\n",
    "\n",
    "y_LR_Testing[\"predictions\"] = LR_result\n",
    "\n",
    "display(y_LR_Testing)\n",
    "\n",
    "# Super-impose known answers to DataFrame.\n",
    "DT_result = pd.concat([DT_result, y_DT_Testing], axis=1)\n",
    "#LR_result = pd.concat([LR_result, y_LR_Testing], axis=1)\n",
    "# GOOD SINCE YOU ARE CONCATENATING PREDS TO LABELS, THE EARLIER RESETTING OF THE INDEX WAS THE RIGHT CHOICE!\n",
    "\n",
    "# Display results.\n",
    "#display(DT_result)\n",
    "#display(LR_result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f927f71",
   "metadata": {},
   "source": [
    "### Testing Accuracy in Practice\n",
    "\n",
    "- Test the classification model and calculate accuracy percentage.\n",
    "- Test the Regression model and calculate accuracy percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788eced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Decision Tree Scores:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy Score: 0.76966'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy Percentage: 76.97%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Linear Regression Scores:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy Score: 0.69384'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy Percentage: 69.38%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset results DataFrames from previous exercise to remove concatenations.\n",
    "DT_result = DT_Model.predict(X_DT_Testing)\n",
    "LR_result = LR_Model.predict(X_LR_Testing)\n",
    "\n",
    "# Gather Accuracy Scores.\n",
    "DT_acc = DT_Model.score(X_DT_Testing, y_DT_Testing)\n",
    "LR_acc = LR_Model.score(X_LR_Testing, y_LR_Testing)\n",
    "\n",
    "# Calculate percentages.\n",
    "DT_per = DT_acc*100\n",
    "LR_per = LR_acc*100\n",
    "\n",
    "# Display scores and percentages.\n",
    "display(\"Decision Tree Scores:\")\n",
    "display(f\"Accuracy Score: {DT_acc:.5f}\")\n",
    "display(f\"Accuracy Percentage: {DT_per:.2f}%\")\n",
    "display(\"\\n\")\n",
    "display(\"Linear Regression Scores:\")\n",
    "display(f\"Accuracy Score: {LR_acc:.5f}\")\n",
    "display(f\"Accuracy Percentage: {LR_per:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
